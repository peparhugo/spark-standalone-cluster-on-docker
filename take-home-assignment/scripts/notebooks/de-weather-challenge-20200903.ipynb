{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 from Assignment\n",
    "1. Load the global weather data into your big data technology of choice.\n",
    "2. Join the stationlist.csv with the countrylist.csv to get the full country name\n",
    "for each station number.\n",
    "3. Join the global weather data with the full country names by station number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "#set app name to the assigment challenge\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"de-weather-challenge\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25306"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1.1 load data\n",
    "station_df = spark.read.csv(path=\"../../data/stationlist.csv\", sep=\",\", header=True)\n",
    "station_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|STN_NO|COUNTRY_ABBR|\n",
      "+------+------------+\n",
      "|012240|          NO|\n",
      "|020690|          SW|\n",
      "|020870|          SW|\n",
      "|021190|          SW|\n",
      "|032690|          UK|\n",
      "|033450|          UK|\n",
      "|039290|          UK|\n",
      "|039790|          EI|\n",
      "|040480|          IC|\n",
      "|041300|          IC|\n",
      "+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get distinct country abbreviations\n",
    "station_df.select(\"COUNTRY_ABBR\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|COUNTRY_ABBR|       COUNTRY_FULL|\n",
      "+------------+-------------------+\n",
      "|          AA|              ARUBA|\n",
      "|          AC|ANTIGUA AND BARBUDA|\n",
      "|          AF|        AFGHANISTAN|\n",
      "|          AG|            ALGERIA|\n",
      "|          AI|   ASCENSION ISLAND|\n",
      "|          AJ|         AZERBAIJAN|\n",
      "|          AL|            ALBANIA|\n",
      "|          AM|            ARMENIA|\n",
      "|          AN|            ANDORRA|\n",
      "|          AO|             ANGOLA|\n",
      "+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_df = spark.read.csv(path=\"../../data/countrylist.csv\", sep=\",\", header=True)\n",
    "country_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding: there are 288 countries in the countrylist.csv but only 251 distinct country abbreviations in stationlist.csv so there are missing countries\n",
    "\n",
    "Assumption: I will use an inner join to join country_df and station_df since all the questions require weather data from stations. If a country does not have a station, then I am assuming it does not have any weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+\n",
      "|STN---| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD| GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+\n",
      "|010260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9| 29.7| 29.8|21.7*|0.02G| 18.5|001000|\n",
      "|010260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6| 22.1|27.1*| 20.7|0.48G| 22.8|001000|\n",
      "|010260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4| 49.5|37.4*|26.8*|0.25G|999.9|011000|\n",
      "|010260|99999|20190104|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8| 10.9| 36.1| 31.8|0.52G|999.9|001000|\n",
      "|010260|99999|20190105|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6| 21.0|38.5*| 32.7|0.02G| 23.6|010000|\n",
      "|010260|99999|20190106|38.5|34.1|1008.2| 994.2| 12.8|10.0| 17.5| 28.9| 41.4|33.8*|0.12G| 23.2|010000|\n",
      "|010260|99999|20190107|32.1|29.8| 996.8| 982.7|  6.9|11.3| 15.5| 28.6|35.1*| 30.4|0.00G|999.9|001000|\n",
      "|010260|99999|20190108|31.6|28.0| 997.4| 983.3| 22.9| 5.9| 11.7| 19.0| 34.3|28.0*|0.53G|  0.4|011000|\n",
      "|010260|99999|20190109|29.9|27.7|1011.6| 997.3| 29.8| 7.6| 15.2| 26.6| 32.4| 26.1|0.20G| 23.6|001000|\n",
      "|010260|99999|20190110|33.1|30.6| 979.1| 965.3|  5.3|17.8| 24.9| 41.8| 41.4|28.8*|0.00G|999.9|011000|\n",
      "|010260|99999|20190111|31.2|29.0| 975.0| 961.1|  5.6|11.6| 17.5| 38.9|33.3*|27.9*|0.83G|  0.4|011100|\n",
      "|010260|99999|20190112|28.3|26.1| 988.2| 974.1|  8.2| 8.1| 13.6| 38.5|30.7*|25.7*|0.00G|999.9|001000|\n",
      "|010260|99999|20190113|22.7|20.9| 977.1| 963.0| 26.6| 4.1|  7.8| 15.2| 27.7|18.0*|0.51G|  0.4|001000|\n",
      "|010260|99999|20190114|20.0|18.3| 984.3| 970.0| 43.1| 3.6|  9.7| 10.7|23.4*| 15.4|0.05G| 38.6|000000|\n",
      "|010260|99999|20190115|25.9|23.2| 991.3| 977.1| 16.0| 7.4| 13.8| 20.8| 27.3| 19.2|0.07G|999.9|001000|\n",
      "|010260|99999|20190116|24.8|21.8| 992.5| 978.2| 33.4| 2.7|  5.8|999.9| 26.1| 23.5|0.06G| 35.4|001000|\n",
      "|010260|99999|20190117|21.4|19.0| 989.8| 975.5| 10.4| 6.1|  8.9| 13.4|24.4*|19.4*|0.04G| 35.0|001000|\n",
      "|010260|99999|20190118|21.0|19.1| 994.4| 980.0| 13.8| 5.6|  7.8| 12.8| 22.3| 19.2|0.11G| 35.0|001000|\n",
      "|010260|99999|20190119|20.2|18.5|1000.8| 986.3| 33.9| 3.4|  7.8| 10.3|22.1*|17.6*|0.09G| 35.8|001000|\n",
      "|010260|99999|20190120|21.7|18.5|1009.0| 994.4| 32.1| 9.5| 14.6| 20.4|23.5*| 18.7|0.11G|999.9|001000|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df = spark.read.csv(path=\"../../data/data/*\", sep=\",\", header=True)\n",
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1.2 join country and station list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
